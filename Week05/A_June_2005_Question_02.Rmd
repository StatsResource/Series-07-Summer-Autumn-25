---
title: "Joint Random Variables: Worked Example"
subtitle: "Discrete Probability Distributions"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(knitr)
library(tidyquant)
library(ggside)
library(ggplot2)


```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent_inverse(
  #base_color = "#3C989E")(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### a) State and prove Bayes' Theorem

**Statement:**
Bayes' Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Mathematically, it’s expressed as:

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
---

**Proof:**
1. Start with the definition of conditional probability:
  $$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
  $$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

2. From the above, we have:
  $$P(A \cap B) = P(A|B)P(B) = P(B|A)P(A)$$

3. Solving for $ P(A|B) $:
  $$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
   
   
---

### b) Joint Probability Mass Function

Given the joint probability mass function, let’s display it in table format:

$$
\begin{array}{c|ccc}
  & Y=1 & Y=2 & Y=3 & Y=4 \\
\hline
X=1 & \frac{1}{12} & \frac{1}{6} & \frac{1}{12} & \frac{1}{3} \\
X=2 & \frac{1}{6} & 0 & \frac{1}{6} & \frac{1}{3} \\
X=3 & 0 & \frac{1}{3} & 0 & \frac{1}{3} \\
X=4 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{2} \\
\end{array}
$$

---

### i) Show that X and Y are dependent

To show $X$ and $Y$ are dependent, we check if the joint probability $ P(X = 2, Y = 3) $ is equal to the product of the marginals $ P(X = 2) $ and $ P(Y = 3) $:

1. From the table:
  $$P(X = 2, Y = 3) = \frac{1}{6}$$
2. Marginal probabilities:
  $$P(X = 2) = \sum_{y} P(X=2, Y=y) = \frac{1}{6} + 0 + \frac{1}{6} + \frac{1}{3} = \frac{5}{12}$$
  $$P(Y = 3) = \sum_{x} P(X=x, Y=3) = \frac{1}{12} + \frac{1}{6} + 0 + \frac{1}{3} = \frac{7}{12}$$

3. Compare the products:
  $$P(X = 2) \cdot P(Y = 3) = \frac{5}{12} \cdot \frac{7}{12} = \frac{35}{144}$$

4. Since $ \frac{1}{6} \neq \frac{35}{144} $, $ X $ and $ Y $ are dependent.


---

### ii) Independent U and V with same marginals

Given marginal distributions:
1. For $X$:
  $$
   \begin{aligned}
   P(U=1) &= \frac{1}{12} + \frac{1}{6} + \frac{1}{12} + \frac{1}{3} = \frac{7}{12} \\
   P(U=2) &= \frac{1}{6} + 0 + \frac{1}{6} + \frac{1}{3} = \frac{5}{12} \\
   \end{aligned}
  $$

2. For $Y$:
  $$
   \begin{aligned}
   P(V=1) &= \frac{1}{12} + \frac{1}{6} + 0 + 0 = \frac{1}{4} \\
   P(V=2) &= \frac{1}{6} + 0 + \frac{1}{3} + \frac{1}{3} = \frac{5}{12} \\
   P(V=3) &= \frac{1}{12} + \frac{1}{6} + 0 + \frac{1}{3} = \frac{7}{12} \\
   P(V=4) &= \frac{1}{3} + \frac{1}{3} + \frac{1}{3} + \frac{1}{2} = \frac{12}{12} \\
   \end{aligned}
  $$

---

Joint distribution assuming independence:
$$
\begin{array}{c|ccc}
  & V=1 & V=2 & V=3 & V=4 \\
\hline
U=1 & \frac{7}{12} \cdot \frac{1}{4} & \frac{7}{12} \cdot \frac{5}{12} & \frac{7}{12} \cdot \frac{7}{12} & \frac{7}{12} \cdot 1 \\
U=2 & \frac{5}{12} \cdot \frac{1}{4} & \frac{5}{12} \cdot \frac{5}{12} & \frac{5}{12} \cdot \frac{7}{12} & \frac{5}{12} \cdot 1 \\
\end{array}
$$


---


---
Voilà! Here’s how we approached and solved each part. If you need more clarification on any step, let me know!